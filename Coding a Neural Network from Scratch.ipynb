{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvj7qtM0GbX4x0VGRALZcI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"VoAAkq4VQufJ","executionInfo":{"status":"ok","timestamp":1757888462029,"user_tz":240,"elapsed":2005,"user":{"displayName":"Sarah Lozina","userId":"04746190378423288707"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","import time as timer"]},{"cell_type":"code","source":["#SELU parameters\n","alpha = 1.6732632423543772848170429916717 #From Pytorch Docs\n","scale = 1.0507009873554804934193349852946 #From Pytorch Docs\n","\n","#Activation Functions\n","\n","#relu returns 0 if the input (Z) is negative, otherwise returns the input as is (no changes)\n","def relu(Z):\n","  return np.maximum(0, Z)\n","\n","#unlike relu, selu keeps the mean and variances of activations close to 0 and 1\n","#this helps keep activations standardized and can prevent vanishing or exploding gradients\n","def selu(Z):\n","  return scale*np.where(Z>0, Z, alpha*(np.exp(Z) - 1))\n","\n","#the softmax activation returns an output vector that is N entries long, with the entry at index i corresponding to the probability of a particular input belonging to the class i\n","def softmax(Z):\n","  expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n","  return expZ/np.sum(expZ, axis=1, keepdims=True)\n","\n","#Activation Derivatives\n","#derivatives of activation functions are crucial for calculating the gradients of the loss function with respect to the network's weights and biases in order to optimize the network's(backpropagation)\n","\n","#derivative of relu is 1 if the input if it is positive and 0 if the input is negative\n","#by doing this the weights and biases for some neurons (when gradient is 0) are not updated\n","def relu_derivative(Z):\n","  return np.where(Z>0, 1, 0)\n","\n","def selu_derivative(Z):\n","  return np.where(Z>0, scale, scale*alpha*np.exp(Z))\n","\n","def softmax_derivative(Z):\n","  s = softmax(Z)\n","  return s*(1 - s)"],"metadata":{"id":"YPeORea1RBfB","executionInfo":{"status":"ok","timestamp":1757888446081,"user_tz":240,"elapsed":11,"user":{"displayName":"Sarah Lozina","userId":"04746190378423288707"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#Networks Class\n","class BasicNetwork:\n","  def __init__(self, input_size, hidden1_size, hidden2_size, output_size, lr=1e-3):\n","    #Initialize Network parameters\n","    self.input_size = input_size\n","    self.hidden1_size = hidden1_size\n","    self.hidden2_size = hidden2_size\n","    self.output_size = output_size\n","    self.lr = lr\n","\n","    #Initialize Weights and Biases\n","    self.W1 = np.random.randn(input_size,  hidden1_size)*0.01\n","    self.b1 = np.zeros((1, hidden1_size))\n","    self.W2 = np.random.randn(hidden1_size, hidden2_size)*0.01\n","    self.b2 = np.zeros((1, hidden2_size))\n","    self.W3 = np.random.randn(hidden2_size, output_size)*0.01\n","    self.b3 = np.zeros((1, output_size))\n","\n","  def forward(self, X):\n","    #Layer 1 is Selu Activation\n","    self.Z1 = X.dot(self.W1) + self.b1\n","    self.A1 = selu(self.Z1)\n","    #Layer 2 is Relu Activation\n","    self.Z2 = self.A1.dot(self.W2) + self.b2\n","    self.A2 = relu(self.Z2)\n","    #Layer 3 is Softmax Activation\n","    self.Z3 = self.A2.dot(self.W3) + self.b3\n","    self.A3 = softmax(self.Z3)\n","    return self.A3\n","\n","  def loss(self, y_pred, y_true):\n","    m = y_pred.shape[0]\n","    y_flat = y_true.reshape(-1)\n","    eps = 1e-8\n","    correct_logprobs = -np.log(y_pred[np.arange(m), y_flat] + eps)\n","    loss = np.sum(correct_logprobs)/m\n","    return loss\n","\n","  def backward(self, X, y):\n","    #backpropagation calculates the errors between the predicted and actual outputs\n","    #gradients are calculated with the derivative of the activation functions\n","    m = X.shape[0]\n","    y_flat = y.reshape(-1)\n","    #Output Layer, Softmax and Cross-Entropy\n","    dZ3 = self.A3.copy()\n","    dZ3[np.arange(m), y_flat] -= 1\n","    dZ3 /= m\n","    dW3 = self.A2.T.dot(dZ3)\n","    db3 = np.sum(dZ3, axis=0, keepdims=True)\n","    #Layer 2 is Relu Activation\n","    dA2 = dZ3.dot(self.W3.T)\n","    dZ2 = dA2*relu_derivative(self.Z2)\n","    dW2 = self.A1.T.dot(dZ2)\n","    db2 = np.sum(dZ2, axis=0, keepdims=True)\n","    #Layer 1 is Selu Activation\n","    dA1 = dZ2.dot(self.W2.T)\n","    dZ1 = dA1*selu_derivative(self.Z1)\n","    dW1 = X.T.dot(dZ1)\n","    db1 = np.sum(dZ1, axis=0, keepdims=True)\n","    return dW1, db1, dW2, db2, dW3, db3\n","\n","  def fit(self, epochs, X_train, y_train):\n","    for epoch in range(1, epochs+1):\n","      self.forward(X_train) #forward pass\n","      loss = self.loss(self.A3, y_train) #calculate loss\n","      dW1, db1, dW2, db2, dW3, db3 = self.backward(X_train, y_train) #calculate gradients\n","      #update weights and biases\n","      self.W1 -= self.lr*dW1\n","      self.b1 -= self.lr*db1\n","      self.W2 -= self.lr*dW2\n","      self.b2 -= self.lr*db2\n","      self.W3 -= self.lr*dW3\n","      self.b3 -= self.lr*db3\n","      #print out training loss\n","      if epoch % 100==0:\n","        print(f\"Epoch [{epoch:04d}/{epochs}], Train Loss: {loss:.4f}\")\n","    return\n","\n","  def predict(self, X):\n","    probs = self.forward(X)\n","    return np.argmax(probs, axis=1)\n","\n","  def score(self, X_test, y_test):\n","    preds = self.predict(X_test)\n","    y_flat = y_test.reshape(-1)\n","    return np.mean(preds==y_flat)"],"metadata":{"id":"GJxi-5y6TIxm","executionInfo":{"status":"ok","timestamp":1757888447824,"user_tz":240,"elapsed":9,"user":{"displayName":"Sarah Lozina","userId":"04746190378423288707"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#Load Dataset\n","url = \"https://raw.githubusercontent.com/dvasiliu/AML/refs/heads/main/Data%20Sets/mobile.csv\"\n","df = pd.read_csv(url)\n","X = df.drop(\"price_range\", axis=1).values.astype(float)\n","y = df[\"price_range\"].values.astype(int)"],"metadata":{"id":"3tsiGbnCUQ2t","executionInfo":{"status":"ok","timestamp":1757888473029,"user_tz":240,"elapsed":310,"user":{"displayName":"Sarah Lozina","userId":"04746190378423288707"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#Start Timer\n","print()\n","t1 = timer.time()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKJrA7teUXRp","executionInfo":{"status":"ok","timestamp":1757888483424,"user_tz":240,"elapsed":42,"user":{"displayName":"Sarah Lozina","userId":"04746190378423288707"}},"outputId":"02f0d76b-5ace-407b-e5ff-06b70cf7f680"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["#Setup KFold Cross-Validation\n","kf = KFold(n_splits=5, shuffle=True)\n","accs = []\n","for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n","  #Split Data and Scale\n","  X_train, X_test = X[train_idx], X[test_idx]\n","  y_train, y_test = y[train_idx], y[test_idx]\n","  scaler = StandardScaler()\n","  X_train = scaler.fit_transform(X_train)\n","  X_test = scaler.transform(X_test)\n","\n","  #Initialize the Network\n","  net = BasicNetwork(input_size=X.shape[1], hidden1_size=64, hidden2_size=32, output_size=4, lr=0.1)\n","\n","  #Train and Score the Network\n","  epochs = 1000\n","  net.fit(epochs=epochs, X_train=X_train, y_train=y_train)\n","  acc = net.score(X_test, y_test)\n","  print(f\"Fold {fold} Accuracy: {acc:.4f}\\n\")\n","  accs.append(acc)\n","\n","#End Timer and Output Stats\n","t2 = timer.time()\n","deltat = t2-t1\n","epochdeltat = deltat/epochs\n","print(f'\\nTraining Took: {deltat//60} min {deltat%60:.2f} s')\n","print(f'Time Per Epoch: {epochdeltat:.4f} s')\n","\n","#Output Results\n","print(f'\\nFold Accuracies: {np.round(accs,4)}')\n","print(f\"Average 5-fold Accuracy: {np.mean(accs):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jSEZgY4UZdV","executionInfo":{"status":"ok","timestamp":1757888614606,"user_tz":240,"elapsed":75930,"user":{"displayName":"Sarah Lozina","userId":"04746190378423288707"}},"outputId":"d60d2f7b-6875-4780-bc2f-0fb8e4dc1a06"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [0100/1000], Train Loss: 1.3858\n","Epoch [0200/1000], Train Loss: 1.3802\n","Epoch [0300/1000], Train Loss: 0.8428\n","Epoch [0400/1000], Train Loss: 0.4759\n","Epoch [0500/1000], Train Loss: 0.3448\n","Epoch [0600/1000], Train Loss: 0.2703\n","Epoch [0700/1000], Train Loss: 0.2182\n","Epoch [0800/1000], Train Loss: 0.1764\n","Epoch [0900/1000], Train Loss: 0.1425\n","Epoch [1000/1000], Train Loss: 0.1158\n","Fold 1 Accuracy: 0.9500\n","\n","Epoch [0100/1000], Train Loss: 1.3861\n","Epoch [0200/1000], Train Loss: 1.3855\n","Epoch [0300/1000], Train Loss: 1.3757\n","Epoch [0400/1000], Train Loss: 0.7436\n","Epoch [0500/1000], Train Loss: 0.4623\n","Epoch [0600/1000], Train Loss: 0.3418\n","Epoch [0700/1000], Train Loss: 0.2703\n","Epoch [0800/1000], Train Loss: 0.2185\n","Epoch [0900/1000], Train Loss: 0.1753\n","Epoch [1000/1000], Train Loss: 0.1431\n","Fold 2 Accuracy: 0.9750\n","\n","Epoch [0100/1000], Train Loss: 1.3857\n","Epoch [0200/1000], Train Loss: 1.3822\n","Epoch [0300/1000], Train Loss: 1.0101\n","Epoch [0400/1000], Train Loss: 0.5068\n","Epoch [0500/1000], Train Loss: 0.3626\n","Epoch [0600/1000], Train Loss: 0.2806\n","Epoch [0700/1000], Train Loss: 0.2226\n","Epoch [0800/1000], Train Loss: 0.1773\n","Epoch [0900/1000], Train Loss: 0.1425\n","Epoch [1000/1000], Train Loss: 0.1165\n","Fold 3 Accuracy: 0.9600\n","\n","Epoch [0100/1000], Train Loss: 1.3860\n","Epoch [0200/1000], Train Loss: 1.3851\n","Epoch [0300/1000], Train Loss: 1.3595\n","Epoch [0400/1000], Train Loss: 0.6526\n","Epoch [0500/1000], Train Loss: 0.4330\n","Epoch [0600/1000], Train Loss: 0.3303\n","Epoch [0700/1000], Train Loss: 0.2697\n","Epoch [0800/1000], Train Loss: 0.2273\n","Epoch [0900/1000], Train Loss: 0.1934\n","Epoch [1000/1000], Train Loss: 0.1649\n","Fold 4 Accuracy: 0.9550\n","\n","Epoch [0100/1000], Train Loss: 1.3859\n","Epoch [0200/1000], Train Loss: 1.3844\n","Epoch [0300/1000], Train Loss: 1.3106\n","Epoch [0400/1000], Train Loss: 0.5809\n","Epoch [0500/1000], Train Loss: 0.3991\n","Epoch [0600/1000], Train Loss: 0.3032\n","Epoch [0700/1000], Train Loss: 0.2396\n","Epoch [0800/1000], Train Loss: 0.1915\n","Epoch [0900/1000], Train Loss: 0.1550\n","Epoch [1000/1000], Train Loss: 0.1275\n","Fold 5 Accuracy: 0.9675\n","\n","\n","Training Took: 2.0 min 11.09 s\n","Time Per Epoch: 0.1311 s\n","\n","Fold Accuracies: [0.95   0.975  0.96   0.955  0.9675]\n","Average 5-fold Accuracy: 0.9615\n"]}]},{"cell_type":"markdown","source":["Sources:\n","\n","[Understanding and implementing relu](https://www.digitalocean.com/community/tutorials/relu-function-in-python)\n","\n","[Understanding and implementing selu](https://medium.com/@iitkarthik/selu-function-in-keras-the-secret-to-self-normalizing-neural-networks-with-practical-examples-f0b3cacc0775)\n","\n","[Understanding and implementing selu derivative](https://neuralthreads.medium.com/selu-and-elu-exponential-linear-units-a826d5eeb99c)\n","\n","[Understanding and implementing softmax and derivative](https://medium.com/intuitionmath/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d)\n","\n","[deciding what weights to initialize](https://medium.com/@shaomukherjee/demystifying-weight-initialization-methods-in-neural-networks-96042d2447f1)\n","\n","[random.normal parameters](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html)\n","\n","[for building the network class](https://www.geeksforgeeks.org/implementation-of-neural-network-from-scratch-using-numpy/)\n","\n","[cross-entropy loss](https://www.geeksforgeeks.org/how-to-implement-softmax-and-cross-entropy-in-python-and-pytorch/)\n","\n","[backpropagation](https://www.geeksforgeeks.org/backpropagation-in-neural-network/)"],"metadata":{"id":"bPz1It-1Pt8T"}}]}